## In questo notebook vediamo le 3 versioni di discesa del gradiente 
* Batch Gradient Descent (che utilizza tutto il dataset per aggiornare i parametri Theta0 e Theta1)
* Stochastic Gradient Descent (che utilizza un esempio per volta per aggiornare i parametri Theta0 e Theta1)
* Mini-batch Gradient Descent (che divide il dataset in gruppi e utilizza un gruppo per volta per aggiornare i parametri Theta0 e Theta1)